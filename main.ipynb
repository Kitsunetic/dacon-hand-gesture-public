{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dc8051-9d3a-496d-b825-114e317ba71b",
   "metadata": {},
   "source": [
    "- You can find the full source code from\n",
    "[github](https://github.com/Kitsunetic/dacon-hand-gesture-public.git).\n",
    "- You can also download the pretrained weights from\n",
    "[here](https://github.com/Kitsunetic/dacon-hand-gesture-public/releases/tag/weights).\n",
    "- This notebook contains full code of dataset generation and model training.\n",
    "It is highly recommended to visit\n",
    "[notebook](https://github.com/Kitsunetic/dacon-hand-gesture-public/blob/master/inference.ipynb)\n",
    "if what you want is only inferencing and reproducing.\n",
    "- Even though you want to do the training steps from scratch,\n",
    "it is highly recommended to clone the github's source code and run `main.py`\n",
    "because this notebook could contains unexpected errors while compressing original source codes into a single notebook.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "## 0-1. Directory Structure\n",
    "\n",
    "```\n",
    "+ data\n",
    "  + ori (original dataset)\n",
    "    + train\n",
    "      - *.png\n",
    "    + test\n",
    "      - *.png\n",
    "    - hand_gesture_pose.csv\n",
    "    - sample_submission.csv\n",
    "  + crop512_9 (the new dataset)\n",
    "    + train\n",
    "      - 000_00_000.png\n",
    "      - 000_00_000.pth\n",
    "      - ...\n",
    "    + test\n",
    "      - 000_00.png\n",
    "      - 000_00.pth\n",
    "      - ...\n",
    "+ const\n",
    "  - __init__.py\n",
    "  - flip.py\n",
    "  - label_names.py\n",
    "- utils.py\n",
    "- main.ipynb (this file)\n",
    "- inference.ipynb (inference notebook file)\n",
    "- main.py (the original training script)\n",
    "```\n",
    "\n",
    "## 0-2. Python Libraries\n",
    "\n",
    "- Albumentations\n",
    "- opencv-python\n",
    "- imageio\n",
    "- numpy\n",
    "- pandas\n",
    "- timm\n",
    "- torch==1.7.0 with cuda toolkit 11.2.2, cudnn8\n",
    "- pyaml\n",
    "- adabelief_pytorch\n",
    "- scikit-learn\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ec83c-f7b5-4b63-99d8-09b66039bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from multiprocessing import Pool\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.models.layers import Conv2dSame\n",
    "from timm.models.nfnet import ScaledStdConv2dSame\n",
    "from torch import Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629daad-65f9-4f5d-8777-6193ee187a45",
   "metadata": {},
   "source": [
    "Please refer to [github link](https://github.com/Kitsunetic/dacon-hand-gesture-public.git)\n",
    "for these three python script files. They define constant variables related to the dataset and basic utility functions.\n",
    "I omitted them from here because it makes the notebook unnecessarily long.\n",
    "\n",
    "- [*const/flip.py*](https://github.com/Kitsunetic/dacon-hand-gesture-public/blob/master/const/flip.py)\n",
    "- [*const/label_names.py*](https://github.com/Kitsunetic/dacon-hand-gesture-public/blob/master/const/label_names.py)\n",
    "- [*utils.py*](https://github.com/Kitsunetic/dacon-hand-gesture-public/blob/master/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e15983-91dd-4b98-a1bb-639d176ac99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from const.flip import id_flip\n",
    "from const.label_names import id_to_label, label_names\n",
    "from utils import AverageMeter, CustomLogger, make_result_dir, seed_everything, tqdm_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b886af6-68f3-4f2c-a08b-5431d7ba7de0",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "\n",
    "Here we will make new dataset by cropping the original dataset that located in `./data/ori` and will locate the new dataset into `./data/crop512_9` directory.\n",
    "The new dataset contains both `*.png` and `*.pth` files.\n",
    "But you can just ignore the `*.pth` files which will not be used in this notebook.\n",
    "\n",
    "First, I cropped and resized the original images into $512 \\times 512$ images with bounding box referencing to the keypoint annotations in the `*.json` files.\n",
    "But this dataset contains some annotation errors.\n",
    "I precisely inspected which data indexes containing mis-annotated data.\n",
    "Bounding box from the keypoint pixels are used at that case, which idea came from 게으름뱅이's codeshare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a0fa6-d08e-4884-a79e-92bda05639a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = (512, 512)\n",
    "crop_padding = 120\n",
    "ratio_limit = 1.2\n",
    "seq_len = 5\n",
    "\n",
    "# data numbers where its keypoints contains error\n",
    "wrong_data = [312, 317, 318, 327, 340, 343, 475, 543, 619, 622, 750, 746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc8cdc-0cfb-4584-b0ea-97bee82492fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62856cdc-bcfe-4d75-9338-7f0b98e9d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"./data/crop512_9\")\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "train_out_dir = out_dir / \"train\"\n",
    "test_out_dir = out_dir / \"test\"\n",
    "train_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42a725-c04c-429d-a6d5-0b5fe6e8d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_resize(im, bbox, dsize, ratio_limit):\n",
    "    \"\"\"resize while keep aspect ratio\"\"\"\n",
    "    # bbox (x1, y1, x2, y2)\n",
    "    # dsize (w, h)\n",
    "    # ratio_limit: float\n",
    "\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "\n",
    "    if h == w:\n",
    "        return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)\n",
    "\n",
    "    long = h > w\n",
    "    a, b = (h, w) if long else (w, h)\n",
    "    ratio = a / b\n",
    "\n",
    "    if ratio <= ratio_limit:\n",
    "        return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)\n",
    "\n",
    "    e, f, g = (bbox[0], bbox[2], im.shape[1]) if long else (bbox[1], bbox[3], im.shape[0])\n",
    "\n",
    "    db = int(a / ratio_limit)\n",
    "    c = db - b\n",
    "    e -= math.ceil(c / 2)\n",
    "    f += math.floor(c / 2)\n",
    "\n",
    "    if e < 0:\n",
    "        f += -e\n",
    "        e = 0\n",
    "    elif f > g:\n",
    "        e -= f - g\n",
    "        f = g\n",
    "\n",
    "    e = max(0, e)\n",
    "    f = min(f, g)\n",
    "\n",
    "    if long:\n",
    "        bbox[0], bbox[2] = e, f\n",
    "    else:\n",
    "        bbox[1], bbox[3] = e, f\n",
    "    fb = f - e\n",
    "\n",
    "    return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b7b9f-ea42-4952-8ec9-9b38ac89f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bbox(im, u):\n",
    "    \"\"\"\n",
    "    refered to 게으름뱅이's codeshare:\n",
    "    https://dacon.io/competitions/official/235805/codeshare/3373\n",
    "    \"\"\"\n",
    "    mask = (im == [255, 0, 0]).all(axis=-1) | (im == [0, 255, 0]).all(axis=-1)\n",
    "\n",
    "    pos = np.stack(mask.nonzero())\n",
    "    bbox = np.round(\n",
    "        np.array(\n",
    "            (\n",
    "                np.clip(pos[1, :].min() - u, 0, 1920),\n",
    "                np.clip(pos[0, :].min() - u, 0, 1920),\n",
    "                np.clip(pos[1, :].max() + u, 0, 1920),\n",
    "                np.clip(pos[0, :].max() + u, 0, 1920),\n",
    "            ),\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "    ).astype(np.int64)\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f077bc-8c0a-4d2f-aa6c-18668216a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(impath: Path, keypoints: np.ndarray):\n",
    "    im = imageio.imread(impath)\n",
    "\n",
    "    # crop\n",
    "    u = crop_padding\n",
    "\n",
    "    if int(impath.parent.name) in wrong_data:\n",
    "        bbox = find_bbox(im, u)\n",
    "    else:\n",
    "        v = keypoints\n",
    "        bbox = np.round(\n",
    "            np.array(\n",
    "                (\n",
    "                    np.clip(v[:, 0].min() - u, 0, 1920),\n",
    "                    np.clip(v[:, 1].min() - u, 0, 1080),\n",
    "                    np.clip(v[:, 0].max() + u, 0, 1920),\n",
    "                    np.clip(v[:, 1].max() + u, 0, 1080),\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        ).astype(np.int64)\n",
    "\n",
    "    im = elastic_resize(im, bbox, dsize, ratio_limit)\n",
    "\n",
    "    # standardization\n",
    "    im2 = (im.astype(np.float32) / 255.0 - imagenet_mean) / imagenet_std\n",
    "    im2 = torch.from_numpy(im2).permute(2, 0, 1).type(torch.float32)\n",
    "\n",
    "    return im, im2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd7ea1-51e2-4725-b597-2b150743d8a2",
   "metadata": {},
   "source": [
    "## 1-1. Generate Training Dataset\n",
    "\n",
    "Generate training dataset in parallel with `multiprocessing` module.\n",
    "\n",
    "The training image file names follow this format `{dir index}_{image index}_{label index}.png`,\n",
    "e.g. `001_02_003.png`, it means the second image in `001` directory and its label index is `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8929a-e1a7-4553-840d-b6643ac70f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir_train(dirpath: Path):\n",
    "    with open(dirpath / f\"{dirpath.name}.json\") as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    diridx = int(dirpath.name)\n",
    "\n",
    "    label = id_to_label[j[\"action\"][0]]\n",
    "    label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    for i, annot in enumerate(j[\"annotations\"]):\n",
    "        impath = dirpath / f\"{i}.png\"\n",
    "        im_org, im = process_image(impath, np.array(annot[\"data\"]))\n",
    "\n",
    "        # save image\n",
    "        fname = f\"{diridx:03d}_{i:02d}_{label.item():03d}\"\n",
    "        imageio.imwrite(train_out_dir / (fname + \".png\"), im_org)\n",
    "        torch.save(im, train_out_dir / (fname + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308304e-5f9b-4740-b003-29a2f8666b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(list(Path(\"./data/ori/train\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026bc81-cb5a-432c-92ed-5fc16ac526d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dirs) # 649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec2ccb-86d0-40e0-a51b-733cb239167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool() as pool:\n",
    "    with tqdm(total=len(dirs), ncols=100, file=sys.stdout) as t:\n",
    "        for _ in pool.imap_unordered(process_dir_train, dirs):\n",
    "            t.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d061d47-daac-4d75-b755-4fb77852544c",
   "metadata": {},
   "source": [
    "## 1-2. Generate Test Dataset\n",
    "\n",
    "The test image file names are following this format `{dir index}_{image index}.png`, e.g. `001_02.png`.\n",
    "It's basically same with the training one but there is no label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0436e6-55e3-4b92-a755-44c3d4b014c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir_test(dirpath: Path):\n",
    "    with open(dirpath / f\"{dirpath.name}.json\") as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    diridx = int(dirpath.name)\n",
    "\n",
    "    for i, annot in enumerate(j[\"annotations\"]):\n",
    "        impath = dirpath / f\"{i}.png\"\n",
    "        im_org, im = process_image(impath, np.array(annot[\"data\"]))\n",
    "\n",
    "        # save image\n",
    "        fname = f\"{diridx:03d}_{i:02d}\"\n",
    "        imageio.imwrite(test_out_dir / (fname + \".png\"), im_org)\n",
    "        torch.save(im, test_out_dir / (fname + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba826f78-9a5e-4078-9bdf-38f9aa964d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(list(Path(\"./data/ori/test\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f905bf-919e-4512-bdb7-ab9876c4282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dirs) # 217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4c966-de44-48d3-95c2-6d64ac0e0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool() as pool:\n",
    "    with tqdm(total=len(dirs), ncols=100, file=sys.stdout) as t:\n",
    "        for _ in pool.imap_unordered(process_dir_test, dirs):\n",
    "            t.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f272f87c-806a-4bde-9f32-a6ffbf1e349f",
   "metadata": {},
   "source": [
    "# 2. Define Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcce676-eda7-481d-927d-bb764ec73004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    https://dacon.io/competitions/official/235585/codeshare/1796\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2.0, eps=1e-7):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        # print(self.gamma)\n",
    "        self.eps = eps\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logp = self.ce(input, target)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240cc7f-fb32-4fa1-b81b-a69aef5e97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    https://github.com/davda54/sam/blob/main/sam.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][\n",
    "            0\n",
    "        ].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "            torch.stack(\n",
    "                [\n",
    "                    p.grad.norm(p=2).to(shared_device)\n",
    "                    for group in self.param_groups\n",
    "                    for p in group[\"params\"]\n",
    "                    if p.grad is not None\n",
    "                ]\n",
    "            ),\n",
    "            p=2,\n",
    "        )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14470ab6-a285-4f7b-af7a-4096c62df3ff",
   "metadata": {},
   "source": [
    "# 3. Define Hyper-parameters and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c12a3-113a-4134-9ca4-0645fdfa5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./data\")\n",
    "DATA_NAME = \"crop512_9\"\n",
    "N_CLASSES = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44bc7b-243b-4487-b7e0-ac78f965fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # experiment\n",
    "    exp_num: str = \"001\"\n",
    "    ver_num: int = None\n",
    "    result_dir_root: PathLike = Path(\"results/exp\")\n",
    "    result_dir: PathLike = None\n",
    "    seed: int = 867243624\n",
    "    debug: bool = False\n",
    "\n",
    "    # network\n",
    "    model_name: str = \"tf_efficientnetv2_l_in21ft1k\"\n",
    "    checkpoint_path: PathLike = None\n",
    "    len_sequence: int = 5\n",
    "    pretrained: bool = True\n",
    "\n",
    "    # criterion\n",
    "    criterion: str = \"focal\"  # ce, focal\n",
    "\n",
    "    # training\n",
    "    num_folds: int = 5\n",
    "    fold: int = 1\n",
    "    earlystop_limit = 10\n",
    "    epochs: int = 100\n",
    "    finetune: bool = True\n",
    "    finetune_step1_epochs: int = 2\n",
    "    finetune_step2_epochs: int = 4\n",
    "\n",
    "    # optimizer / scheduler\n",
    "    optimizer_name: str = \"AdaBelief\"\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.01\n",
    "    scheduler: Any = ReduceLROnPlateau\n",
    "\n",
    "    # Sharpness-Aware Minimization for Efficiently Improving Generalization [2020]\n",
    "    sam: bool = True  # no simultaneous with LA, it have higher priority\n",
    "\n",
    "    # Lookahead Optimizer: k steps forward, 1 step back [2019]\n",
    "    look_ahead: bool = False\n",
    "    look_ahead_k: int = 5\n",
    "    look_ahead_alpha: float = 0.5\n",
    "\n",
    "    # dataoader\n",
    "    batch_size: int = 50\n",
    "    num_workers: int = None\n",
    "\n",
    "    # dataset\n",
    "    in_memory: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.debug:\n",
    "            self.epochs = 1\n",
    "            self.finetune = False\n",
    "\n",
    "        if self.num_workers is None:\n",
    "            self.num_workers = self.batch_size * self.len_sequence // 10\n",
    "\n",
    "    def to_yaml(self, target):\n",
    "        data = yaml.load(str(self.__dict__), Loader=yaml.FullLoader)\n",
    "        with open(str(target), \"w\") as f:\n",
    "            yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89297e5-8210-4987-9ae0-05823395c446",
   "metadata": {},
   "source": [
    "# 4. Prepare Items for Training\n",
    "\n",
    "## 4-1. Construct NN\n",
    "\n",
    "I converted the input sequence images into multi-channel single image by stacking them,\n",
    "i.e. converted 5 images with shape of `(3, height, width)` into single image with shape of `(15, height, width)`.\n",
    "And replaced the first CNN of the pre-trained network so that it can handle 15 channel input image.\n",
    "After that I filled the CNN's weight and bias data with stack of copied original weights to exploit pre-trained weights.\n",
    "For example, the origin CNN's weight have shape of `(out_channels, 3, kernel_height, kernel_width)`.\n",
    "I have repeated it 5 times and stacked them all to form the weight's shape of `(out_channels, 15, kernel_height, kernel_width)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6c31d-182c-4c37-bd31-a6def05e9511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = \"resnet\",\n",
    "        pretrained: bool = True,\n",
    "        n_classes: int = 1000,\n",
    "        len_sequence: int = 5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(name, pretrained=pretrained)\n",
    "\n",
    "        self.tuning_modules: List[nn.Module] = []\n",
    "\n",
    "        self.freeze_step = 3\n",
    "\n",
    "        embedding_size = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Linear(embedding_size, n_classes)\n",
    "        self.tuning_modules.append(self.backbone.classifier)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            c1: Conv2dSame = self.backbone.conv_stem\n",
    "            w = c1.weight.data\n",
    "\n",
    "            c2 = Conv2dSame(3 * len_sequence, c1.out_channels, c1.kernel_size, c1.stride)\n",
    "            c2.weight.data = torch.repeat_interleave(w, len_sequence, dim=1)\n",
    "\n",
    "            self.backbone.conv_stem = c2\n",
    "            # self.tuning_modules.append(c2)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def freeze(self, step=3):\n",
    "        if self.freeze_step != step:\n",
    "            self.freeze_step = step\n",
    "\n",
    "            if step == 1:\n",
    "                self.backbone.requires_grad_(False)\n",
    "                for m in self.tuning_modules:\n",
    "                    m.requires_grad_(True)\n",
    "            elif step == 2:\n",
    "                self.backbone.requires_grad_(True)\n",
    "                for m in self.tuning_modules:\n",
    "                    m.requires_grad_(False)\n",
    "            else:\n",
    "                self.backbone.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8467c6-ed18-4a53-825d-417662030e5e",
   "metadata": {},
   "source": [
    "## 4-2. Load Dataset\n",
    "\n",
    "I randomly applied affine transform and gaussian blur, image compression augmentations during training steps.\n",
    "Horizontal flip augmentations was applied randomly and replaced its label's left-right attribute,\n",
    "i.e. gesture from left hand will be gesture from right hand when it horizontally flipped,\n",
    "and gesture from both hands will keep its label even though it is horizontally flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616b1f4-c03c-4137-9a66-4ca6c923ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLoader:\n",
    "    def __init__(self, in_memory: bool, files: List[Path]) -> None:\n",
    "        self.in_memory = in_memory\n",
    "        self.files = files\n",
    "\n",
    "        if self.in_memory:\n",
    "            self.data = {}\n",
    "            for file in tqdm(self.files, ncols=100, file=sys.stdout, desc=\"in-memory loading...\"):\n",
    "                self.data[file] = imageio.imread(file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, file: Path):\n",
    "        if self.in_memory:\n",
    "            return self.data[file]\n",
    "        else:\n",
    "            return imageio.imread(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e217da5-b62e-4f42-ab1e-dd7f4d36b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureDataset(Dataset):\n",
    "    def __init__(self, items, fileloader: FileLoader, augmentation=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.items = items\n",
    "        self.fileloader = fileloader\n",
    "        self.has_label = len(self.items[0]) == 3\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        self._p = lambda: random.random() > 0.5\n",
    "\n",
    "        t = []\n",
    "        if augmentation:\n",
    "            t.append(A.Affine(scale=(0.9, 1.1), translate_px=(-40, 40), rotate=(-15, 15), shear=(-10, 10)))\n",
    "            t.append(A.GaussianBlur())\n",
    "            t.append(A.ImageCompression())\n",
    "        t.append(A.Normalize())\n",
    "        t.append(ToTensorV2())\n",
    "        self.transform = A.Compose(transforms=t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        files = item[0]\n",
    "        diridx = torch.tensor(item[1], dtype=torch.long)\n",
    "        # ims = torch.cat([torch.load(file) for file in files])  # (L*3, H, W)\n",
    "        # ims = torch.cat([self.transform(image=imageio.imread(file))[\"image\"] for file in files])\n",
    "        ims = torch.cat([self.transform(image=self.fileloader[file])[\"image\"] for file in files])\n",
    "\n",
    "        if self.has_label:\n",
    "            label = item[2]\n",
    "\n",
    "            # flipping\n",
    "            if self.augmentation:\n",
    "                ims, label = self.augment(ims, label)\n",
    "\n",
    "            label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            return ims, diridx, label\n",
    "        else:\n",
    "            return ims, diridx\n",
    "\n",
    "    def fliplr(self, ims: Tensor, label: int):\n",
    "        if label in id_flip:\n",
    "            new_label = id_flip[label]\n",
    "            new_ims = torch.flip(ims, dims=(2,))\n",
    "            return new_ims, new_label\n",
    "        else:\n",
    "            return ims, label\n",
    "\n",
    "    def augment(self, ims: Tensor, label: int):\n",
    "        # fliplr\n",
    "        if self._p():\n",
    "            ims, label = self.fliplr(ims, label)\n",
    "\n",
    "        return ims, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be545662-f49c-4a3f-8bba-d658d4680964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(config: Config):\n",
    "    print(\"Load datasets ...\")\n",
    "\n",
    "    # load train dataset\n",
    "    files_train = sorted(list((DATA_DIR / DATA_NAME / \"train\").glob(\"*.png\")))\n",
    "    files_test = sorted(list((DATA_DIR / DATA_NAME / \"test\").glob(\"*.png\")))\n",
    "    if config.debug:\n",
    "        files_train = files_train[:1000]\n",
    "\n",
    "    fileloader = FileLoader(in_memory=config.in_memory, files=files_train + files_test)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    labels = {}\n",
    "    for file in files_train:\n",
    "        # train filename: {diridx:3d}_{fileidx:2d}_{label:3d}\n",
    "        diridx = int(file.stem[:3])\n",
    "        data[diridx].append(file)\n",
    "        labels[diridx] = int(file.stem[-3:])\n",
    "\n",
    "    items_file = []\n",
    "    items_dir = []\n",
    "    items_label = []\n",
    "    for diridx in data:\n",
    "        if len(data[diridx]) > config.len_sequence:\n",
    "            for start_idx in range(len(data[diridx]) - config.len_sequence + 1):\n",
    "                items_file.append(data[diridx][start_idx : start_idx + config.len_sequence])\n",
    "                items_dir.append(diridx)\n",
    "                items_label.append(labels[diridx])\n",
    "        elif len(data[diridx]) == config.len_sequence:\n",
    "            items_file.append(data[diridx])\n",
    "            items_dir.append(diridx)\n",
    "            items_label.append(labels[diridx])\n",
    "        else:\n",
    "            fake = [data[diridx][-1] for _ in range(config.len_sequence - len(data[diridx]))]\n",
    "            items_file.append(data[diridx] + fake)\n",
    "            items_dir.append(diridx)\n",
    "            items_label.append(labels[diridx])\n",
    "\n",
    "    label_cnt = defaultdict(list)\n",
    "    for i, label in enumerate(items_label):\n",
    "        label_cnt[label].append(i)\n",
    "\n",
    "    skf = StratifiedKFold(config.num_folds, shuffle=True, random_state=config.seed)\n",
    "    tidx, vidx = list(skf.split(items_file, items_label))[config.fold - 1]\n",
    "\n",
    "    items_train = [(items_file[i], items_dir[i], items_label[i]) for i in tidx]\n",
    "    items_valid = [(items_file[i], items_dir[i], items_label[i]) for i in vidx]\n",
    "\n",
    "    ds_train = GestureDataset(items_train, fileloader=fileloader, augmentation=True)\n",
    "    ds_valid = GestureDataset(items_valid, fileloader=fileloader, augmentation=False)\n",
    "\n",
    "    dl_kwargs = dict(batch_size=config.batch_size, num_workers=config.num_workers, pin_memory=True)\n",
    "    dl_train = DataLoader(ds_train, **dl_kwargs, shuffle=True)\n",
    "    dl_valid = DataLoader(ds_valid, **dl_kwargs, shuffle=False)\n",
    "\n",
    "    # load test dataset\n",
    "    data = defaultdict(list)\n",
    "    for file in files_test:\n",
    "        # test filename: {diridx:3d}_{fileidx:2d}\n",
    "        diridx = int(file.stem[:3])\n",
    "        data[diridx].append(file)\n",
    "\n",
    "    items_test = []\n",
    "    for diridx in data:\n",
    "        if len(data[diridx]) > config.len_sequence:\n",
    "            for i in range(len(data[diridx]) - config.len_sequence):\n",
    "                items_test.append((data[diridx][i : i + config.len_sequence], diridx))\n",
    "        elif len(data[diridx]) == config.len_sequence:\n",
    "            items_test.append((data[diridx], diridx))\n",
    "        else:\n",
    "            fake = [data[diridx][-1] for _ in range(config.len_sequence - len(data[diridx]))]\n",
    "            items_test.append((data[diridx] + fake, diridx))\n",
    "\n",
    "    ds_test = GestureDataset(items_test, fileloader=fileloader, augmentation=False)\n",
    "    dl_test = DataLoader(ds_test, **dl_kwargs, shuffle=False)\n",
    "\n",
    "    return dl_train, dl_valid, dl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8fd1d-2400-4a31-90c9-2dd1487dda12",
   "metadata": {},
   "source": [
    "## 4-3. Make Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6624e8-a548-4d82-96bb-ea4f331be6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureTrainerOutput:\n",
    "    def __init__(self) -> None:\n",
    "        self._loss = AverageMeter()\n",
    "        self._correct, self._total = 0, 0\n",
    "        self._preds, self._targets = [], []\n",
    "        self._labels = list(range(N_CLASSES))\n",
    "\n",
    "        self._target_names = [label_names[i][\"name\"] for i in range(N_CLASSES)]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, loss: Tensor, preds: Tensor, labels: Tensor):\n",
    "        n = preds.size(0)\n",
    "        self._loss.update(loss.item(), n)\n",
    "\n",
    "        pclass = preds if preds.dim() == 1 else preds.argmax(dim=1)\n",
    "        tclass = labels if labels.dim() == 1 else labels.argmax(dim=1)\n",
    "\n",
    "        self._correct += (pclass == tclass).sum().item()\n",
    "        self._total += n\n",
    "\n",
    "        self._preds.extend(pclass.tolist())\n",
    "        self._targets.extend(tclass.tolist())\n",
    "\n",
    "    @property\n",
    "    def acc(self):\n",
    "        if self._total == 0:\n",
    "            return 0\n",
    "        return self._correct / self._total * 100\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss()\n",
    "\n",
    "    @property\n",
    "    def f1(self):\n",
    "        return f1_score(self._targets, self._preds, labels=self._labels, average=\"macro\")\n",
    "\n",
    "    @property\n",
    "    def report(self):\n",
    "        return classification_report(\n",
    "            self._targets,\n",
    "            self._preds,\n",
    "            labels=self._labels,\n",
    "            target_names=self._target_names,\n",
    "        )\n",
    "\n",
    "\n",
    "class GestureTrainer:\n",
    "    def __init__(self, config: Config) -> None:\n",
    "        self.config = config\n",
    "        self.log = CustomLogger(config.result_dir / \"log.log\")\n",
    "        self.log_rpt = CustomLogger(config.result_dir / \"report_train.log\")\n",
    "        self.log_rpv = CustomLogger(config.result_dir / \"report_valid.log\")\n",
    "\n",
    "        self.epoch = 1\n",
    "        self.best_loss = math.inf\n",
    "        self.earlystop_cnt = 0\n",
    "\n",
    "        self.model = Net(\n",
    "            name=config.model_name,\n",
    "            n_classes=N_CLASSES,\n",
    "            pretrained=config.pretrained,\n",
    "            len_sequence=config.len_sequence,\n",
    "        ).cuda()\n",
    "\n",
    "        if config.checkpoint_path is not None:\n",
    "            self.model.load_state_dict(torch.load(config.checkpoint_path))\n",
    "\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        # criterion\n",
    "        if config.criterion == \"ce\":\n",
    "            self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "        elif config.criterion == \"focal\":\n",
    "            self.criterion = FocalLoss().cuda()\n",
    "        else:\n",
    "            raise NotImplementedError(config.criterion)\n",
    "\n",
    "        # optimizer\n",
    "        OptimizerClass = {\n",
    "            \"AdamW\": AdamW,\n",
    "            \"AdaBelief\": AdaBelief,\n",
    "        }[config.optimizer_name]\n",
    "        if config.sam:\n",
    "            self.optimizer = SAM(\n",
    "                self.model.parameters(),\n",
    "                base_optimizer=OptimizerClass,\n",
    "                lr=config.lr,\n",
    "                weight_decay=config.weight_decay,\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = OptimizerClass(\n",
    "                self.model.parameters(),\n",
    "                lr=config.lr,\n",
    "                weight_decay=config.weight_decay,\n",
    "            )\n",
    "            if config.look_ahead:\n",
    "                self.optimizer = Lookahead(\n",
    "                    optimizer=self.optimizer,\n",
    "                    k=config.look_ahead_k,\n",
    "                    alpha=config.look_ahead_alpha,\n",
    "                )\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "        self.tdl, self.vdl, self.dl_test = make_datasets(config)\n",
    "\n",
    "    def save(self):\n",
    "        ckpt_path = self.config.result_dir / \"best.pth\"\n",
    "        self.log.info(\"Save checkpoint:\", ckpt_path)\n",
    "\n",
    "        state_dict = self.model.module.state_dict()\n",
    "\n",
    "        torch.save(state_dict, ckpt_path)\n",
    "\n",
    "    def train_loop(self, dl: DataLoader):\n",
    "        o = GestureTrainerOutput()\n",
    "\n",
    "        tqdm_desc = f\"Train [{self.epoch:02d}/{self.config.epochs:02d}]\"\n",
    "\n",
    "        with tqdm(total=len(dl.dataset), **tqdm_kwargs, desc=tqdm_desc) as t:\n",
    "            for images, diridxes, labels in dl:\n",
    "                images_ = images.cuda()\n",
    "                labels_ = labels.cuda()\n",
    "                n = images.size(0)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                logits_ = self.model(images_)\n",
    "                loss = self.criterion(logits_, labels_)\n",
    "\n",
    "                o.update(loss, logits_, labels_)\n",
    "                t.set_postfix_str(f\"loss:{o.loss:.6f}, acc:{o.acc:.2f}\")\n",
    "\n",
    "                loss.backward()\n",
    "                if self.config.sam:\n",
    "                    self.optimizer.first_step(zero_grad=True)\n",
    "                    logits2_ = self.model(images_)\n",
    "                    loss2 = self.criterion(logits2_, labels_)\n",
    "                    loss2.backward()\n",
    "                    self.optimizer.second_step(zero_grad=True)\n",
    "                else:\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                # self.scheduler.step()\n",
    "\n",
    "                t.update(n)\n",
    "\n",
    "        return o\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def valid_loop(self, dl: DataLoader):\n",
    "        o = GestureTrainerOutput()\n",
    "\n",
    "        tqdm_desc = f\"Valid [{self.epoch:02d}/{self.config.epochs:02d}]\"\n",
    "\n",
    "        with tqdm(total=len(dl.dataset), **tqdm_kwargs, desc=tqdm_desc) as t:\n",
    "            for images, diridxes, labels in dl:\n",
    "                images_ = images.cuda()\n",
    "                labels_ = labels.cuda()\n",
    "                n = images.size(0)\n",
    "\n",
    "                logits_ = self.model(images_)\n",
    "                loss = self.criterion(logits_, labels_)\n",
    "\n",
    "                o.update(loss, logits_, labels_)\n",
    "                t.set_postfix_str(f\"loss:{o.loss:.6f}, acc:{o.acc:.2f}\")\n",
    "\n",
    "                t.update(n)\n",
    "\n",
    "        return o\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def callback(self, to: GestureTrainerOutput, vo: GestureTrainerOutput):\n",
    "        print()\n",
    "\n",
    "        self.log.info(\n",
    "            f\"ep[{self.epoch:03d}/{self.config.epochs:03d}]\",\n",
    "            f\"loss[{to.loss:.6f};{vo.loss:.6f}]\",\n",
    "            f\"acc[{to.acc:.2f};{vo.acc:.2f}]\",\n",
    "            f\"f1[{to.f1:.4f};{vo.f1:.4f}]\",\n",
    "        )\n",
    "\n",
    "        _t = f\"ep[{self.epoch:03d}/{self.config.epochs:03d}]\"\n",
    "        self.log_rpt.info(\"TRAIN\", _t, \"\\n\", to.report)\n",
    "        self.log_rpv.info(\"VALID\", _t, \"\\n\", vo.report)\n",
    "\n",
    "        if isinstance(self.scheduler, ReduceLROnPlateau):\n",
    "            self.scheduler.step(vo.loss)\n",
    "\n",
    "        if self.best_loss > vo.loss:\n",
    "            self.best_loss = vo.loss\n",
    "            self.earlystop_cnt = 0\n",
    "            self.save()\n",
    "        else:\n",
    "            self.earlystop_cnt += 1\n",
    "\n",
    "        self.log.flush()\n",
    "        self.log_rpt.flush()\n",
    "        self.log_rpv.flush()\n",
    "\n",
    "    def fit(self):\n",
    "        seed_everything(self.config.seed)\n",
    "\n",
    "        for self.epoch in range(self.epoch, self.config.epochs + 1):\n",
    "            if self.config.finetune:\n",
    "                if self.epoch <= self.config.finetune_step1_epochs:\n",
    "                    self.model.module.freeze(step=1)\n",
    "                elif self.epoch <= self.config.finetune_step2_epochs:\n",
    "                    self.model.module.freeze(step=2)\n",
    "                else:\n",
    "                    self.model.module.freeze(step=3)\n",
    "\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            to = self.train_loop(self.tdl)\n",
    "\n",
    "            if self.earlystop_cnt >= self.config.earlystop_limit:\n",
    "                self.log.info(\"Early Stopping\")\n",
    "                break\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    vo = self.valid_loop(self.vdl)\n",
    "                    self.callback(to, vo)\n",
    "\n",
    "            self.log.flush()\n",
    "            self.log_rpt.flush()\n",
    "            self.log_rpv.flush()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def submit(self):\n",
    "        # load best checkpoint\n",
    "        checkpoint_path = self.config.result_dir / \"best.pth\"\n",
    "        self.log.info(\"Load checkpoint\", checkpoint_path)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.module.load_state_dict(checkpoint)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # exp022: update submission tta\n",
    "        ret = defaultdict(list)\n",
    "        with tqdm(total=len(self.dl_test.dataset), ncols=100, file=sys.stdout, desc=\"submission\") as t:\n",
    "            for images, diridxes in self.dl_test:\n",
    "                logits = self.model(images.cuda()).cpu()\n",
    "\n",
    "                for logit, diridx in zip(logits, diridxes):\n",
    "                    ret[diridx.item()].append(logit)\n",
    "\n",
    "                    t.update()\n",
    "\n",
    "        out_sm = defaultdict(list)\n",
    "        out_ms = defaultdict(list)\n",
    "        for diridx, logits in ret.items():\n",
    "            out_sm[\"Image_Path\"].append(f\"./test\\\\{diridx}\")\n",
    "            out_ms[\"Image_Path\"].append(f\"./test\\\\{diridx}\")\n",
    "\n",
    "            logits = torch.stack(logits)\n",
    "            logit_sm = logits.softmax(dim=1).mean(dim=0)\n",
    "            logit_ms = logits.mean(dim=0).softmax(dim=0)\n",
    "\n",
    "            for k in range(196):\n",
    "                if k in id_to_label:\n",
    "                    out_sm[f\"Label_{k}\"].append(logit_sm[id_to_label[k]].item())\n",
    "                    out_ms[f\"Label_{k}\"].append(logit_ms[id_to_label[k]].item())\n",
    "\n",
    "        df_sm = pd.DataFrame(out_sm)\n",
    "        df_ms = pd.DataFrame(out_ms)\n",
    "\n",
    "        out_df_path = str(self.config.result_dir / f\"exp{self.config.exp_num}_ver{self.config.ver_num}_%s.csv\")\n",
    "        print(\"Write result to\", out_df_path % \"_\")\n",
    "        df_sm.to_csv(out_df_path % \"sm\", index=False)\n",
    "        df_ms.to_csv(out_df_path % \"ms\", index=False) # ms was better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d07f8-df6d-4394-a830-71e83bb47ce0",
   "metadata": {},
   "source": [
    "# 5. Train Neural Network\n",
    "\n",
    "The output log and weight files will be located in `./results/exp/exp001/version_{config.ver_num}/`.\n",
    "The `config.ver_num` will be automatically updated each time when you running this script.\n",
    "\n",
    "I made my final ensembled submission by calculating average from 17 other inference submissions where each training was performed under different seed and fold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59ab03-21e8-4741-b3e2-aa9eaaa50929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config = Config(\n",
    "        debug=False,\n",
    "        finetune=True,\n",
    "        model_name=\"tf_efficientnetv2_l_in21ft1k\",\n",
    "        batch_size=18,\n",
    "        sam=True,\n",
    "        pretrained=True,\n",
    "        optimizer_name=\"AdaBelief\",\n",
    "        fold=1,\n",
    "        seed=1,\n",
    "        num_workers=6,\n",
    "        in_memory=True,\n",
    "        lr=1e-3,\n",
    "    )\n",
    "    \n",
    "    config.result_dir = make_result_dir(config)\n",
    "    shutil.copy(__file__, config.result_dir / Path(__file__).name)\n",
    "    config.to_yaml(config.result_dir / \"params.yaml\")\n",
    "\n",
    "    trainer = GestureTrainer(config)\n",
    "    trainer.fit()\n",
    "    trainer.submit()\n",
    "\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
