{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5b0416-2d26-406e-9da9-c86a1eacf269",
   "metadata": {},
   "source": [
    "This notebook is exactly same with dataset generation step in `inference.ipynb` and `main.ipynb`.\n",
    "\n",
    "# 0. Prerequisites\n",
    "\n",
    "## 0-1. Python Libraries\n",
    "\n",
    "- Albumentations\n",
    "- opencv-python\n",
    "- imageio\n",
    "- numpy\n",
    "- pandas\n",
    "- timm\n",
    "- torch==1.7.0 with cuda toolkit 11.2.2, cudnn8\n",
    "- pyaml\n",
    "- adabelief_pytorch\n",
    "- scikit-learn\n",
    "- tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7679173c-1b34-4b45-a663-57818f8d02bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from multiprocessing import Pool\n",
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple, List\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from adabelief_pytorch import AdaBelief\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from timm.models.layers import Conv2dSame\n",
    "from timm.models.nfnet import ScaledStdConv2dSame\n",
    "from torch import Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9040599-d5e1-4f03-a3a6-3dbb9ddb63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from const.flip import id_flip\n",
    "from const.label_names import id_to_label, label_names\n",
    "from utils import AverageMeter, CustomLogger, make_result_dir, seed_everything, tqdm_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f3ecba-26e1-43df-b50f-0a902939317a",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "\n",
    "The original dataset is located in `./data/ori`.\n",
    "And I will make new dataset by cropping the original dataset and it will be located in `./data/crop512_9`.\n",
    "\n",
    "The new dataset contains both `*.png` and `*.pth` files.\n",
    "But the `*.pth` files are not used in this code, so you can just ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc05cbe1-eb01-4188-8c20-49a98f41943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsize = (512, 512)\n",
    "crop_padding = 120\n",
    "ratio_limit = 1.2\n",
    "seq_len = 5\n",
    "\n",
    "# data numbers where its keypoints contains error\n",
    "wrong_data = [312, 317, 318, 327, 340, 343, 475, 543, 619, 622, 750, 746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a11e722c-c116-41ed-a62f-36a639b9254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1488590-421c-4611-9431-b5572678fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"./data/crop512_9\")\n",
    "if out_dir.exists():\n",
    "    shutil.rmtree(out_dir)\n",
    "\n",
    "train_out_dir = out_dir / \"train\"\n",
    "test_out_dir = out_dir / \"test\"\n",
    "train_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "test_out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daba193a-8187-4d9e-bf29-bdac9de9c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_resize(im, bbox, dsize, ratio_limit):\n",
    "    \"\"\"resize while keep aspect ratio\"\"\"\n",
    "    # bbox (x1, y1, x2, y2)\n",
    "    # dsize (w, h)\n",
    "    # ratio_limit: float\n",
    "\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "\n",
    "    if h == w:\n",
    "        return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)\n",
    "\n",
    "    long = h > w\n",
    "    a, b = (h, w) if long else (w, h)\n",
    "    ratio = a / b\n",
    "\n",
    "    if ratio <= ratio_limit:\n",
    "        return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)\n",
    "\n",
    "    e, f, g = (bbox[0], bbox[2], im.shape[1]) if long else (bbox[1], bbox[3], im.shape[0])\n",
    "\n",
    "    db = int(a / ratio_limit)\n",
    "    c = db - b\n",
    "    e -= math.ceil(c / 2)\n",
    "    f += math.floor(c / 2)\n",
    "\n",
    "    if e < 0:\n",
    "        f += -e\n",
    "        e = 0\n",
    "    elif f > g:\n",
    "        e -= f - g\n",
    "        f = g\n",
    "\n",
    "    e = max(0, e)\n",
    "    f = min(f, g)\n",
    "\n",
    "    if long:\n",
    "        bbox[0], bbox[2] = e, f\n",
    "    else:\n",
    "        bbox[1], bbox[3] = e, f\n",
    "    fb = f - e\n",
    "\n",
    "    return cv2.resize(im[bbox[1] : bbox[3], bbox[0] : bbox[2]], dsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca17b8b-2ae1-4c27-9c60-ff73f65450ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bbox(im, u):\n",
    "    \"\"\"\n",
    "    refer to 게으름뱅이 code share:\n",
    "        https://dacon.io/competitions/official/235805/codeshare/3373?page=1&dtype=recent\n",
    "    \"\"\"\n",
    "    mask = (im == [255, 0, 0]).all(axis=-1) | (im == [0, 255, 0]).all(axis=-1)\n",
    "\n",
    "    pos = np.stack(mask.nonzero())\n",
    "    bbox = np.round(\n",
    "        np.array(\n",
    "            (\n",
    "                np.clip(pos[1, :].min() - u, 0, 1920),\n",
    "                np.clip(pos[0, :].min() - u, 0, 1920),\n",
    "                np.clip(pos[1, :].max() + u, 0, 1920),\n",
    "                np.clip(pos[0, :].max() + u, 0, 1920),\n",
    "            ),\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "    ).astype(np.int64)\n",
    "\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2adafa88-dfd8-4b52-a863-08d6d8a4a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(impath: Path, keypoints: np.ndarray):\n",
    "    im = imageio.imread(impath)\n",
    "\n",
    "    # crop\n",
    "    u = crop_padding\n",
    "\n",
    "    if int(impath.parent.name) in wrong_data:\n",
    "        bbox = find_bbox(im, u)\n",
    "    else:\n",
    "        v = keypoints\n",
    "        bbox = np.round(\n",
    "            np.array(\n",
    "                (\n",
    "                    np.clip(v[:, 0].min() - u, 0, 1920),\n",
    "                    np.clip(v[:, 1].min() - u, 0, 1080),\n",
    "                    np.clip(v[:, 0].max() + u, 0, 1920),\n",
    "                    np.clip(v[:, 1].max() + u, 0, 1080),\n",
    "                ),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "        ).astype(np.int64)\n",
    "\n",
    "    im = elastic_resize(im, bbox, dsize, ratio_limit)\n",
    "\n",
    "    # standardization\n",
    "    im2 = (im.astype(np.float32) / 255.0 - imagenet_mean) / imagenet_std\n",
    "    im2 = torch.from_numpy(im2).permute(2, 0, 1).type(torch.float32)\n",
    "\n",
    "    return im, im2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa39ee4a-cd34-4e69-9178-52a6a555fc8e",
   "metadata": {},
   "source": [
    "## 1-1. Generate Training Dataset\n",
    "\n",
    "The training image file names are following this format `{dir index}_{image index}_{label index}.png`, e.g. `001_02_003.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b61152fb-09f7-429d-919e-225e79b68676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir_train(dirpath: Path):\n",
    "    with open(dirpath / f\"{dirpath.name}.json\") as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    diridx = int(dirpath.name)\n",
    "\n",
    "    label = id_to_label[j[\"action\"][0]]\n",
    "    label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    for i, annot in enumerate(j[\"annotations\"]):\n",
    "        impath = dirpath / f\"{i}.png\"\n",
    "        im_org, im = process_image(impath, np.array(annot[\"data\"]))\n",
    "\n",
    "        # save image\n",
    "        fname = f\"{diridx:03d}_{i:02d}_{label.item():03d}\"\n",
    "        imageio.imwrite(train_out_dir / (fname + \".png\"), im_org)\n",
    "        torch.save(im, train_out_dir / (fname + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63a9c7c9-d849-4619-aba2-4bba3c4e4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(list(Path(\"./data/ori/train\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08b529e0-b611-4dde-98b7-95f1d5399aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "649"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dirs)  # 649"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e503c00-cf77-4e1c-bc5f-a10d00eb42e7",
   "metadata": {},
   "source": [
    "There are 649 training directories containing multiple images.\n",
    "I applied parallelism because processing all of these data takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "400054e0-f964-4ab8-b830-958b774bd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 649/649 [12:01<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "with Pool() as pool:\n",
    "    with tqdm(total=len(dirs), ncols=100, file=sys.stdout) as t:\n",
    "        for _ in pool.imap_unordered(process_dir_train, dirs):\n",
    "            t.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9c7e8-8f9d-4a25-a6d4-cd07752d00ab",
   "metadata": {},
   "source": [
    "## 1-2. Generate Test Dataset\n",
    "\n",
    "The test image file names are following this format `{dir index}_{image index}_{label index}.png`, e.g. `001_02.png`.\n",
    "It's basically similar to training dataset but there is no label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59cc5d9d-cead-4335-ad59-86ad88c4876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dir_test(dirpath: Path):\n",
    "    with open(dirpath / f\"{dirpath.name}.json\") as f:\n",
    "        j = json.load(f)\n",
    "\n",
    "    diridx = int(dirpath.name)\n",
    "\n",
    "    for i, annot in enumerate(j[\"annotations\"]):\n",
    "        impath = dirpath / f\"{i}.png\"\n",
    "        im_org, im = process_image(impath, np.array(annot[\"data\"]))\n",
    "\n",
    "        # save image\n",
    "        fname = f\"{diridx:03d}_{i:02d}\"\n",
    "        imageio.imwrite(test_out_dir / (fname + \".png\"), im_org)\n",
    "        torch.save(im, test_out_dir / (fname + \".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59bb663b-9dcf-4310-90e9-bc1aa1ecd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = sorted(list(Path(\"./data/ori/test\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a529fe6a-ee9b-4989-8c04-bd238443e301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dirs)  # 217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0e795b-0f15-4a13-b004-2d6053cfed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 217/217 [04:15<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "with Pool() as pool:\n",
    "    with tqdm(total=len(dirs), ncols=100, file=sys.stdout) as t:\n",
    "        for _ in pool.imap_unordered(process_dir_test, dirs):\n",
    "            t.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
